{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear example notebook\n",
    "\n",
    "In this notebook, we'll go throuhg some exampels on how to use `alma`, as well as how one can configure it to one's own use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## benchmark_model\n",
    "\n",
    "`benchmark_model` is the core API for `alma`. It allows one to benchmark's one's model speed on given data for as all of the conversion options that `alma` supports.\n",
    "\n",
    "We'll start with just initializing a model, and creating some data we'll pasd through it for our benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict\n",
    "\n",
    "import torch\n",
    "\n",
    "from alma.utils.setup_logging import setup_logging\n",
    "\n",
    "# Set up logging. This will be sidscussed for in a later section.\n",
    "setup_logging(log_file=None, level=\"INFO\")\n",
    "\n",
    "# Set the device one wants to benchmark on\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Create a random model\n",
    "model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(3, 3),\n",
    "        torch.nn.ReLU(),\n",
    "    )\n",
    "\n",
    "# Create a random tensor\n",
    "data = torch.rand(1, 512, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can start messing around with `alma`. We'll begin with defining a very simple benchmark config, which will tell `alma` how you would like to benchmark your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [model.py:40] Multiprocessing is enabled, and the model is a torch.nn.Module. This is not memory efficient,\n",
      "as the model will be pickled and sent to each child process, which will require the model to be stored in memory\n",
      "twice. If the model is large, this may cause memory issues. Consider using a callable to return the model, which\n",
      "will be created in each child process, rather than the parent process. See `examples/mnist/mem_efficient_benchmark_rand_tensor.py`\n",
      "for an example.\n",
      "INFO: [benchmark_model.py:112] Benchmarking model using conversion: EAGER\n",
      "INFO: [device.py:163] Chosen device: mps (Fallback selection)\n",
      "Benchmarking EAGER on mps:  94%|█████████▍| 15/16 [00:00<00:00, 1201.62it/s]\n",
      "INFO: [benchmark_model.py:112] Benchmarking model using conversion: JIT_TRACE\n",
      "INFO: [device.py:163] Chosen device: mps (Fallback selection)\n",
      "Benchmarking JIT_TRACE on mps:  94%|█████████▍| 15/16 [00:00<00:00, 1435.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All results:\n",
      "EAGER results:\n",
      "Device: mps\n",
      "Total elapsed time: 0.0142 seconds\n",
      "Total inference time (model only): 0.0010 seconds\n",
      "Total samples: 1024 - Batch size: 64\n",
      "Throughput: 1030868.68 samples/second\n",
      "\n",
      "\n",
      "JIT_TRACE results:\n",
      "Device: mps\n",
      "Total elapsed time: 0.0114 seconds\n",
      "Total inference time (model only): 0.0011 seconds\n",
      "Total samples: 1024 - Batch size: 64\n",
      "Throughput: 966456.23 samples/second\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from alma.benchmark import BenchmarkConfig\n",
    "from alma.benchmark.log import display_all_results\n",
    "from alma.benchmark_model import benchmark_model\n",
    "\n",
    "# Set up the benchmarking configuration\n",
    "config = BenchmarkConfig(\n",
    "    n_samples=1024,  # Total nb of samples to benchmark on\n",
    "    batch_size=64,  # Batch size\n",
    "    device=device,  # The device to benchmark on\n",
    ")\n",
    "\n",
    "# What conversion methods to benchmark. In this case, it is just \"eager\", which is the default forward call, and \n",
    "# \"jit trace\", which is a jit-traced model.\n",
    "conversions = [\"EAGER\", \"JIT_TRACE\"]\n",
    "\n",
    "# Benchmark the model\n",
    "results: Dict[str, Dict[str, Any]] = benchmark_model(\n",
    "    model, config, conversions, data=data.squeeze()  # The batch dimension should be squeezed away\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "display_all_results(\n",
    "    results, display_function=print, include_errors=True, include_traceback_for_errors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's that simple! You can also feed in the config as a dict if you prefer, but using the `BenchmarkConfig` will give you integrated type hinting. For example, one could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [model.py:40] Multiprocessing is enabled, and the model is a torch.nn.Module. This is not memory efficient,\n",
      "as the model will be pickled and sent to each child process, which will require the model to be stored in memory\n",
      "twice. If the model is large, this may cause memory issues. Consider using a callable to return the model, which\n",
      "will be created in each child process, rather than the parent process. See `examples/mnist/mem_efficient_benchmark_rand_tensor.py`\n",
      "for an example.\n",
      "INFO: [benchmark_model.py:112] Benchmarking model using conversion: EAGER\n",
      "INFO: [device.py:163] Chosen device: mps (Fallback selection)\n",
      "Benchmarking EAGER on mps:  94%|█████████▍| 15/16 [00:00<00:00, 1360.67it/s]\n",
      "INFO: [benchmark_model.py:112] Benchmarking model using conversion: JIT_TRACE\n",
      "INFO: [device.py:163] Chosen device: mps (Fallback selection)\n",
      "Benchmarking JIT_TRACE on mps:  94%|█████████▍| 15/16 [00:00<00:00, 1396.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All results:\n",
      "EAGER results:\n",
      "Device: mps\n",
      "Total elapsed time: 0.0124 seconds\n",
      "Total inference time (model only): 0.0009 seconds\n",
      "Total samples: 1024 - Batch size: 64\n",
      "Throughput: 1152399.16 samples/second\n",
      "\n",
      "\n",
      "JIT_TRACE results:\n",
      "Device: mps\n",
      "Total elapsed time: 0.0116 seconds\n",
      "Total inference time (model only): 0.0011 seconds\n",
      "Total samples: 1024 - Batch size: 64\n",
      "Throughput: 974388.06 samples/second\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up the benchmarking configuration\n",
    "config = {\n",
    "    \"n_samples\": 1024,  # Total nb of samples to benchmark on\n",
    "    \"batch_size\": 64,  # Batch size\n",
    "    \"device\": device,  # The device to benchmark on\n",
    "}\n",
    "\n",
    "conversions = [\"EAGER\", \"JIT_TRACE\"]\n",
    "\n",
    "# Benchmark the model\n",
    "results: Dict[str, Dict[str, Any]] = benchmark_model(\n",
    "    model, config, conversions, data=data.squeeze()\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "display_all_results(\n",
    "    results, display_function=print, include_errors=True, include_traceback_for_errors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the config\n",
    "\n",
    "Now let's dig in a little deeper into the `config` options. If we print out all of the fields, we see this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== BenchmarkConfig Fields ====================\n",
      "\n",
      "Field: n_samples; Type int; Default: 128; Description: Number of samples to benchmark.\n",
      "\n",
      "Field: batch_size; Type int; Default: 128; Description: Batch size for benchmarking.\n",
      "\n",
      "Field: multiprocessing; Type bool; Default: True; Description: Enable multiprocessing support.\n",
      "\n",
      "Field: fail_on_error; Type bool; Default: True; Description: Fail immediately on any error.\n",
      "\n",
      "Field: allow_device_override; Type bool; Default: True; Description: Allow device override selection.\n",
      "\n",
      "Field: allow_cuda; Type bool; Default: True; Description: Allow CUDA acceleration if available.\n",
      "\n",
      "Field: allow_mps; Type bool; Default: True; Description: Allow MPS acceleration if available.\n",
      "\n",
      "Field: device; Type Optional; Default: None; Description: Device for benchmarking.\n"
     ]
    }
   ],
   "source": [
    "def print_pydantic_fields(model_class):\n",
    "    \"\"\"\n",
    "    Print all fields of a Pydantic model class in a pretty format.\n",
    "    Usage: print_pydantic_fields(YourModelClass)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} {model_class.__name__} Fields {'='*20}\")\n",
    "    \n",
    "    for name, field in model_class.model_fields.items():\n",
    "        field_type = field.annotation.__name__ if hasattr(field.annotation, '__name__') else str(field.annotation)\n",
    "        default = field.default if field.default is not ... else \"Required\"\n",
    "        \n",
    "        print(f\"\\nField: {name}; Type {field_type}; Default: {default}; Description: {field.description}\")\n",
    "\n",
    "print_pydantic_fields(BenchmarkConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see some new fields:\n",
    "- multiprocessing\n",
    "- fail_on_error\n",
    "- allow_device_override\n",
    "- allow_cuda\n",
    "- allow_mps\n",
    "\n",
    "`multiprocessing` is a boolean that defines whether or not we should run each conversion method benchmarking of a child process. This means that we spin up a new Python interpreter instance (internally inside of `alma`) for each conversion method, and this allows each method to not affect the others. As we were developing `alma`, we noticed that some conversion methods (e.g. `optimum quanto`) affect the global torch state, and multiprocessing was the solution we came up with for isolating each methods's environment. It is True by default, however fell free to turn it off, especially if debugging!\n",
    "\n",
    "`fail_on_error` just defines whether we fail gracefully or not. Some conversion methods will not work on certain hardware, or because of missing dependencies, etc. We can either stop as soon as we encounter an error, or keep going. If we keep going, the error message and traceback will be returned.\n",
    "\n",
    "`allow_device_override` is a boolean that defines whether or not we will allow `alma` to move conversion methods to specific devices, if the conversion method in question only works on that device. E.g. `ONNX_CPU` will fail on GPU, as will PyTorch's native converted quantized models which are CPU only: `NATIVE_CONVERT_AI8WI8_STATIC_QUANTIZED`. This is `True` by default, but it is very much up to the user. If you want the methods to fail if not compatiblewith `device`, then set this to `False`. If you want `alma` to automatically move the method to the appropriate device, leave it as `True`.\n",
    "\n",
    "`allow_cuda` and `allow_mps` are guides on which device to fallback to in case `device` fails to run the conversion method in question. If `allow_cuda=True` and CUDA is available, then it will default to cuda. If not, then it will similarly check `mps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [model.py:40] Multiprocessing is enabled, and the model is a torch.nn.Module. This is not memory efficient,\n",
      "as the model will be pickled and sent to each child process, which will require the model to be stored in memory\n",
      "twice. If the model is large, this may cause memory issues. Consider using a callable to return the model, which\n",
      "will be created in each child process, rather than the parent process. See `examples/mnist/mem_efficient_benchmark_rand_tensor.py`\n",
      "for an example.\n",
      "INFO: [benchmark_model.py:112] Benchmarking model using conversion: EAGER\n",
      "INFO: [device.py:163] Chosen device: mps (Fallback selection)\n",
      "Benchmarking EAGER on mps:  94%|█████████▍| 15/16 [00:00<00:00, 1201.23it/s]\n",
      "INFO: [benchmark_model.py:112] Benchmarking model using conversion: JIT_TRACE\n",
      "INFO: [device.py:163] Chosen device: mps (Fallback selection)\n",
      "Benchmarking JIT_TRACE on mps:  94%|█████████▍| 15/16 [00:00<00:00, 2218.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All results:\n",
      "EAGER results:\n",
      "Device: mps\n",
      "Total elapsed time: 0.0139 seconds\n",
      "Total inference time (model only): 0.0011 seconds\n",
      "Total samples: 1024 - Batch size: 64\n",
      "Throughput: 904560.08 samples/second\n",
      "\n",
      "\n",
      "JIT_TRACE results:\n",
      "Device: mps\n",
      "Total elapsed time: 0.0076 seconds\n",
      "Total inference time (model only): 0.0010 seconds\n",
      "Total samples: 1024 - Batch size: 64\n",
      "Throughput: 1056079.46 samples/second\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the benchmarking configuration\n",
    "config = BenchmarkConfig(\n",
    "    n_samples=1024,  # Total nb of samples to benchmark on\n",
    "    batch_size=64,  # Batch size\n",
    "    device=device,  # The device to benchmark on\n",
    "    multiprocessing=True,  # If True, we test each method in its own isolated environment,\n",
    "    # which helps keep methods from contaminating the global torch state\n",
    "    fail_on_error=False,  # If False, we fail gracefully and keep testing other methods\n",
    "    allow_device_override=False,  # No overriding of device for any conversion method\n",
    "    allow_cuda=True,  # Does nothing without `allow_device_override`\n",
    "    allow_mps=True,  # Does nothing without `allow_device_override`\n",
    ")\n",
    "\n",
    "# Benchmark the model\n",
    "results: Dict[str, Dict[str, Any]] = benchmark_model(\n",
    "    model, config, conversions, data=data.squeeze()\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "display_all_results(\n",
    "    results, display_function=print, include_errors=True, include_traceback_for_errors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing all conversion methods\n",
    "\n",
    "If one sets `conversions=None`, then by default all of the supported conversion methods will be tested. To see all of the supported methods, one can import them. Printing them will show that eahc option has a name, and an optional device_override field that tells us if there is only a specific hardware that it runs on and that it should move to if `allow_device_override=True` in the `config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: mode='EAGER' device_override=None\n",
      "1: mode='EXPORT+EAGER' device_override=None\n",
      "2: mode='ONNX_CPU' device_override='CPU'\n",
      "3: mode='ONNX_GPU' device_override='CUDA'\n",
      "4: mode='ONNX+DYNAMO_EXPORT' device_override=None\n",
      "5: mode='COMPILE_CUDAGRAPHS' device_override='CUDA'\n",
      "6: mode='COMPILE_INDUCTOR_DEFAULT' device_override=None\n",
      "7: mode='COMPILE_INDUCTOR_REDUCE_OVERHEAD' device_override=None\n",
      "8: mode='COMPILE_INDUCTOR_MAX_AUTOTUNE' device_override=None\n",
      "9: mode='COMPILE_INDUCTOR_EAGER_FALLBACK' device_override=None\n",
      "10: mode='COMPILE_ONNXRT' device_override='CUDA'\n",
      "11: mode='COMPILE_OPENXLA' device_override='XLA_GPU'\n",
      "12: mode='COMPILE_TVM' device_override=None\n",
      "13: mode='EXPORT+AI8WI8_FLOAT_QUANTIZED' device_override=None\n",
      "14: mode='EXPORT+AI8WI8_FLOAT_QUANTIZED+RUN_DECOMPOSITION' device_override=None\n",
      "15: mode='EXPORT+AI8WI8_STATIC_QUANTIZED' device_override=None\n",
      "16: mode='EXPORT+AI8WI8_STATIC_QUANTIZED+RUN_DECOMPOSITION' device_override=None\n",
      "17: mode='EXPORT+AOT_INDUCTOR' device_override=None\n",
      "18: mode='EXPORT+COMPILE_CUDAGRAPHS' device_override='CUDA'\n",
      "19: mode='EXPORT+COMPILE_INDUCTOR_DEFAULT' device_override=None\n",
      "20: mode='EXPORT+COMPILE_INDUCTOR_REDUCE_OVERHEAD' device_override=None\n",
      "21: mode='EXPORT+COMPILE_INDUCTOR_MAX_AUTOTUNE' device_override=None\n",
      "22: mode='EXPORT+COMPILE_INDUCTOR_DEFAULT_EAGER_FALLBACK' device_override=None\n",
      "23: mode='EXPORT+COMPILE_ONNXRT' device_override='CUDA'\n",
      "24: mode='EXPORT+COMPILE_OPENXLA' device_override='XLA_GPU'\n",
      "25: mode='EXPORT+COMPILE_TVM' device_override=None\n",
      "26: mode='NATIVE_CONVERT_AI8WI8_STATIC_QUANTIZED' device_override=None\n",
      "27: mode='NATIVE_FAKE_QUANTIZED_AI8WI8_STATIC' device_override=None\n",
      "28: mode='COMPILE_TENSORRT' device_override=None\n",
      "29: mode='EXPORT+COMPILE_TENSORRT' device_override=None\n",
      "30: mode='JIT_TRACE' device_override=None\n",
      "31: mode='TORCH_SCRIPT' device_override=None\n",
      "32: mode='OPTIMIM_QUANTO_AI8WI8' device_override=None\n",
      "33: mode='OPTIMIM_QUANTO_AI8WI4' device_override=None\n",
      "34: mode='OPTIMIM_QUANTO_AI8WI2' device_override=None\n",
      "35: mode='OPTIMIM_QUANTO_WI8' device_override=None\n",
      "36: mode='OPTIMIM_QUANTO_WI4' device_override=None\n",
      "37: mode='OPTIMIM_QUANTO_WI2' device_override=None\n",
      "38: mode='OPTIMIM_QUANTO_Wf8E4M3N' device_override=None\n",
      "39: mode='OPTIMIM_QUANTO_Wf8E4M3NUZ' device_override=None\n",
      "40: mode='OPTIMIM_QUANTO_Wf8E5M2' device_override=None\n",
      "41: mode='OPTIMIM_QUANTO_Wf8E5M2+COMPILE_CUDAGRAPHS' device_override='cuda'\n"
     ]
    }
   ],
   "source": [
    "from alma.conversions.conversion_options import MODEL_CONVERSION_OPTIONS\n",
    "\n",
    "for index, value in MODEL_CONVERSION_OPTIONS.items():\n",
    "    print(f\"{index}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error handling\n",
    "Let's see what happens if we fail gracefully with an error. This example should fail for everybody, where we set the device as cuda but attempt to run the `NATIVE_CONVERT_AI8WI8_STATIC_QUANTIZED` method which is CPU only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [model.py:40] Multiprocessing is enabled, and the model is a torch.nn.Module. This is not memory efficient,\n",
      "as the model will be pickled and sent to each child process, which will require the model to be stored in memory\n",
      "twice. If the model is large, this may cause memory issues. Consider using a callable to return the model, which\n",
      "will be created in each child process, rather than the parent process. See `examples/mnist/mem_efficient_benchmark_rand_tensor.py`\n",
      "for an example.\n",
      "INFO: [benchmark_model.py:112] Benchmarking model using conversion: EAGER\n",
      "INFO: [device.py:163] Chosen device: mps (Fallback selection)\n",
      "Benchmarking EAGER on mps:  94%|█████████▍| 15/16 [00:00<00:00, 1111.11it/s]\n",
      "INFO: [benchmark_model.py:112] Benchmarking model using conversion: NATIVE_CONVERT_AI8WI8_STATIC_QUANTIZED\n",
      "INFO: [device.py:163] Chosen device: mps (Fallback selection)\n",
      "ERROR: [benchmark_model.py:139] Benchmarking conversion NATIVE_CONVERT_AI8WI8_STATIC_QUANTIZED failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All results:\n",
      "EAGER results:\n",
      "Device: mps\n",
      "Total elapsed time: 0.0148 seconds\n",
      "Total inference time (model only): 0.0010 seconds\n",
      "Total samples: 1024 - Batch size: 64\n",
      "Throughput: 1042321.93 samples/second\n",
      "\n",
      "\n",
      "NATIVE_CONVERT_AI8WI8_STATIC_QUANTIZED results:\n",
      "Benchmarking failed\n",
      "Error: The operator 'aten::_fake_quantize_learnable_per_channel_affine' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set up the benchmarking configuration\n",
    "config = BenchmarkConfig(\n",
    "    n_samples=1024,  # Total nb of samples to benchmark on\n",
    "    batch_size=64,  # Batch size\n",
    "    device=torch.device(\"cuda\"),  # The device to benchmark on\n",
    "    multiprocessing=True,  # If True, we test each method in its own isolated environment,\n",
    "    # which helps keep methods from contaminating the global torch state\n",
    "    fail_on_error=False,  # If False, we fail gracefully and keep testing other methods\n",
    "    allow_device_override=False,  # No overriding of device for any conversion method\n",
    "    allow_cuda=True,  # Does nothing without `allow_device_override`\n",
    "    allow_mps=True,  # Does nothing without `allow_device_override`\n",
    ")\n",
    "\n",
    "# We choose a conversion method that cannot work on GPU\n",
    "conversions = [\"EAGER\", \"NATIVE_CONVERT_AI8WI8_STATIC_QUANTIZED\"]\n",
    "\n",
    "# Benchmark the model\n",
    "results: Dict[str, Dict[str, Any]] = benchmark_model(\n",
    "    model, config, conversions, data=data.squeeze()\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "display_all_results(\n",
    "    results, display_function=print, include_errors=True, include_traceback_for_errors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `NATIVE_CONVERT_AI8WI8_STATIC_QUANTIZED` method failed. If we want to get full details on why, we can access the traceback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/9v/chg43rl50lqc_17y8_ym7vzc0000gn/T/ipykernel_17892/1276288923.py\", line 18, in <module>\n",
      "    results: Dict[str, Dict[str, Any]] = benchmark_model(\n",
      "  File \"/Users/oscarsavolainen/Coding/Mine/Alma-Saif/src/alma/benchmark_model.py\", line 125, in benchmark_model\n",
      "    result = benchmark_process_wrapper(\n",
      "  File \"/Users/oscarsavolainen/Coding/Mine/Alma-Saif/src/alma/utils/multiprocessing/multiprocessing.py\", line 43, in run_benchmark_process\n",
      "    result = benchmark_func(device, *args, **kwargs)\n",
      "  File \"/Users/oscarsavolainen/Coding/Mine/Alma-Saif/src/alma/benchmark/benchmark.py\", line 22, in benchmark\n",
      "    benchmark(\n",
      "  File \"/Users/oscarsavolainen/Coding/Mine/Alma-Saif/src/alma/utils/multiprocessing/error_handling.py\", line 35, in wrapper\n",
      "    result: dict = decorated_func(*args, **kwargs)\n",
      "  File \"/Users/oscarsavolainen/Coding/Mine/Alma-Saif/src/alma/benchmark/benchmark.py\", line 77, in benchmark\n",
      "    forward_call = select_forward_call_function(model, conversion, data, device)\n",
      "  File \"/Users/oscarsavolainen/Coding/Mine/Alma-Saif/src/alma/conversions/select.py\", line 219, in select_forward_call_function\n",
      "    forward = get_converted_quantized_model_forward_call(model, data)\n",
      "  File \"/Users/oscarsavolainen/Coding/Mine/Alma-Saif/src/alma/conversions/options/quant_convert.py\", line 38, in get_converted_quantized_model_forward_call\n",
      "    model_quantized = convert_fx(fx_model)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py\", line 612, in convert_fx\n",
      "    return _convert_fx(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/torch/ao/quantization/quantize_fx.py\", line 540, in _convert_fx\n",
      "    quantized = convert(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/torch/ao/quantization/fx/convert.py\", line 1219, in convert\n",
      "    convert_weighted_module(\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/torch/ao/quantization/fx/convert.py\", line 846, in convert_weighted_module\n",
      "    weight_post_process(float_module.weight)  # type: ignore[operator]\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/alma/lib/python3.10/site-packages/torch/ao/quantization/_learnable_fake_quantize.py\", line 183, in forward\n",
      "    X = torch._fake_quantize_learnable_per_channel_affine(\n",
      "NotImplementedError: The operator 'aten::_fake_quantize_learnable_per_channel_affine' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results[\"NATIVE_CONVERT_AI8WI8_STATIC_QUANTIZED\"][\"traceback\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more succinct message, we can also just access the error message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operator 'aten::_fake_quantize_learnable_per_channel_affine' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.\n"
     ]
    }
   ],
   "source": [
    "print(results[\"NATIVE_CONVERT_AI8WI8_STATIC_QUANTIZED\"][\"error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better use of memory when multiprocessing\n",
    "\n",
    "We do allow people to feed in models directly into `benchmark_model`. However, if multi-processing is enabled, this is not very memory efficient. This is because the model gets intialised as one creates it, and then gets copied over to the child process for each conversion method. This means it can get stored in memory twice. As such, it would be better, if multiprocessing is enabled, to not feed in the model directly. Instead, we can feed in a callable that RETURNS the model. This allows us to only initialize the model inside the child processes, and not in the parent process.\n",
    "\n",
    "Unfortunately, Jupyter notebooks don't play very nicely with multiprocessing, and so we have to refer you to one of our script-based examples, e.g. `examples/mnist/mem_efficient_benchmark_rand_tensor.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a data loader inside of a tensor\n",
    "\n",
    "`alma` provides two options for feeding in data to benchmark the model on. Throuhgout this notebook, we've just fed in a `data` tensor. Under the hood, this initializes a data loader with the config-defined batch size, and then uses that data loader to benchmark the model.\n",
    "\n",
    "However, you might wish to provide your own data loader. In which case, you can provide one via the `data_loader` argument. The config-defined batch size will be overridden.\n",
    "\n",
    "Unfortunately, Jupyter notebooks still don't play very nicely with multiprocessing, and so as in the `get_model` case, we have to refer you to one of our script-based examples, e.g. `examples/mnist/benchmark_with_dataloader.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "We do highly recommend that users set up logging. There are many internal operations that one can get insight into via enabling logging. A `setup_logging` function is provided for convenience, but one\n",
    "can use whatever logging one wishes, or none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [279899324.py:22] Benchmarking model using random data\n",
      "WARNING: [model.py:40] Multiprocessing is enabled, and the model is a torch.nn.Module. This is not memory efficient,\n",
      "as the model will be pickled and sent to each child process, which will require the model to be stored in memory\n",
      "twice. If the model is large, this may cause memory issues. Consider using a callable to return the model, which\n",
      "will be created in each child process, rather than the parent process. See `examples/mnist/mem_efficient_benchmark_rand_tensor.py`\n",
      "for an example.\n",
      "INFO: [benchmark_model.py:112] Benchmarking model using conversion: EAGER\n",
      "INFO: [device.py:163] Chosen device: mps (Fallback selection)\n",
      "Benchmarking EAGER on mps:  94%|█████████▍| 15/16 [00:00<00:00, 1257.03it/s]\n",
      "INFO: [benchmark_model.py:112] Benchmarking model using conversion: JIT_TRACE\n",
      "INFO: [device.py:163] Chosen device: mps (Fallback selection)\n",
      "Benchmarking JIT_TRACE on mps:  94%|█████████▍| 15/16 [00:00<00:00, 1236.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All results:\n",
      "EAGER results:\n",
      "Device: mps\n",
      "Total elapsed time: 0.0136 seconds\n",
      "Total inference time (model only): 0.0010 seconds\n",
      "Total samples: 1024 - Batch size: 64\n",
      "Throughput: 1004334.13 samples/second\n",
      "\n",
      "\n",
      "JIT_TRACE results:\n",
      "Device: mps\n",
      "Total elapsed time: 0.0133 seconds\n",
      "Total inference time (model only): 0.0013 seconds\n",
      "Total samples: 1024 - Batch size: 64\n",
      "Throughput: 764869.20 samples/second\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import Any, Dict\n",
    "\n",
    "import torch\n",
    "\n",
    "from alma.utils.setup_logging import setup_logging\n",
    "\n",
    "\n",
    "# Set up logging. DEBUG level will also log the internal conversion logs (where available), as well\n",
    "# as the model graphs. A `setup_logging` function is provided for convenience, but one can use\n",
    "# whatever logging one wishes, or none.\n",
    "setup_logging(log_file=None, level=\"INFO\")\n",
    "\n",
    "# Set the device one wants to benchmark on\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Benchmark the model\n",
    "# Feeding in a tensor, and no dataloader, will cause the benchmark_model function to generate a\n",
    "# dataloader that provides random tensors of the same shape as `data`, which is used to\n",
    "# benchmark the model. As verbose logging is provided, it will log the benchmarking\n",
    "# at a DEBUG level.\n",
    "logging.info(\"Benchmarking model using random data\")\n",
    "results: Dict[str, Dict[str, Any]] = benchmark_model(\n",
    "    model, config, conversions, data=data.squeeze()\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "display_all_results(\n",
    "    results, display_function=print, include_traceback_for_errors=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the conversion methods have extremely verbose logging. We have opted to wrap most of them\n",
    "in a `suppress_output` context manager that silences all `sys.stdout` and `sys.stderr`. However, if one\n",
    "sets ones logging level to DEBUG with the `setup_logging` function, then those internal import logs\n",
    "will not be supressed.\n",
    "\n",
    "Furthermore, as we have highlighted prior, we provide a `display_all_results` function to print \n",
    "the results in a nice format.There is also a `save_dict_to_json` function to save the results to a \n",
    "JSON file for easy CI integration.\n",
    "\n",
    "### Debugging\n",
    "If one is debugging, it is highly recommended that one use the `setup_logging` function and set one's\n",
    "level to DEBUG. This will, among other things, log any torch.compile warnings and errors thrown by\n",
    "torch.inductor that can point to issues in triton kernels, give verbose ONNX logging, and print \n",
    "the model graphs where appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further examples\n",
    "\n",
    "For script-based examples, including examples on how to use our provided argparser for a CLI, see the `.py` file examples in `example/linear` and `examples/mnist`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
